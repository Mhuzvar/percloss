{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of Perceptually Relevant Metrics of Audio Similarity for Potential Use as Loss In Training Neural Networks\n",
    "\n",
    "There are several areas that need thorough exploration:\n",
    "- conventional loss functions\n",
    "   - equipped with some modification to improve their perceptual relevance\n",
    "   - like pre-emphasis\n",
    "- complex perception focused metrics\n",
    "   - mostly focused on quality assessment\n",
    "   - may need some modification as well\n",
    "- trained models\n",
    "   - least \"safe\" method but likely best performing *as long as the inputs are similar to training dataset*\n",
    "\n",
    "General notes from the literature:\n",
    "\n",
    "- Amos Tversky researched similarity, perception, and categorization from psychology point of view. He noted human perception does not satisfy the definition of a euclidean metric [[1]](#references).\n",
    "- Large portion of \"music similarity\" research focuses on clustering music with the aim of content delivery optimization [[1]](#references).\n",
    "- MFCC based distance may be helpful [[1]](#references) (not directly mentioned) and there should be a way to make mel-cepstral distance differentiable [[2]](#references).\n",
    "- \n",
    "\n",
    "Personal comments:\n",
    "\n",
    "- Before settling for any similarity metric we first need to decide, whether the compared signals have to be produced with the same input signal\n",
    "   - A guitar player may be able to tell if two systems are similar (or the same) even when hearing two different riffs played through them\n",
    "   - Metric which does not require the same input signals on both compared systems may be helpful in some cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Loss\n",
    "\n",
    "- Pros\n",
    "   - easy to use\n",
    "   - easy to represent\n",
    "   - works the same regardless of data\n",
    "   - not computationally demanding\n",
    "- Cons\n",
    "   - perceptually irrelevant\n",
    "   - relevance can only be improved to a limited degree\n",
    "\n",
    "Conventional loss (MSE, log-cosh,...) works well for general tasks, but is completely unrelated to human perception. However it might be usable when improved with some form of pre-emphasis (A-filtering?).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Perception Focused Metrics\n",
    "\n",
    "- Pros\n",
    "   - perceptually relevant\n",
    "   - possible (though possibly difficult) to represent\n",
    "- Cons\n",
    "   - may not be easy to use\n",
    "   - could be computationally too complex\n",
    "\n",
    "### Notes From Literature\n",
    "\n",
    "- Objective models for prediction of pereptual quality can be divided into two categories: parameter-based (ITU-T G.107) and signal-based [[4]](#references).\n",
    "   - Since parameter-based models like ITU-T G.107 are based on modeling communication line characteristics [[4, 5]](#references), only signal-based parameters are considered in our research.\n",
    "   - Signal-based models can be further divided into no-reference and full-reference [[4]](#references). *Only full-reference models are considered for our use case.*\n",
    "\n",
    "- PEMO-Q [[3]](#references)\n",
    "   - PEMO-Q was created for lossy compression evaluation [[3]](#references).\n",
    "   - Attempts to answer the problem of doubts about PEAQ being a realistic and valid model of general auditory perception [[3]](#references).\n",
    "   - Auditory model\n",
    "      - The signals are preprocessed in a way that may not be suitable for our problem [[3]](#references).\n",
    "      - After preprocessing, the signals are transformed into \"internal representation\" using an auditory signal processing model [[3]](#references).\n",
    "         - 35-band gammatone filterbank (basilar membrane characteristics) with each band then processed individually\n",
    "         - half-wave rectification and low pass filter at 1 kHz (transformation of mechanical oscillations to neural firing rates of the inner haircells)\n",
    "         - absolute hearing threshold determined from assumed maximum signal input\n",
    "         - sequence of five nonliear feedback loops\n",
    "            - dividing element and a low-pass (RC)\n",
    "            - input is divided by low passed output\n",
    "         - linear modulation filterbank, the most significant difference from previous work\n",
    "            - a simplified version of PEMO-Q replaces this part with modulation-low-pass version of the auditory model (less accurate but less computationally difficult)\n",
    "      - Lastly, cognitive effects are modeled in post-processing stage [[3]](#references).\n",
    "   - Each channel of the outputs of auditory model are then cross correlated, which (after a weighed sum) gives a perceptual quality measure called PSM [[3]](#references).\n",
    "   - PEMO-Q also defines a second, more detailed (in time) measure called PSM<sub>t</sub>, which is likely not of significance for our work [[3]](#references).\n",
    "   - When using the computationally less demanding version, PEMO-Q is signal dependent [[3]](#references).\n",
    "   - The correlation between the PSM and subjective ratings is higher than between PSM<sub>t</sub> and subjective ratings as long as only one type of signals is studied [[3]](#references).\n",
    "   - According to the paper, it should be applicable more generally than PEAQ, but is not suitable for predicting impact of linear systems [[3]](#references).\n",
    "\n",
    "- ViSQOLAudio [[4], [6], [7]](#references)\n",
    "   - ViSQOL is an objective speech quality signal-based full-reference model published in 2015 [[6]](#references) and last updated in 2020 [[7]](#references).\n",
    "   - It was later modified to create a more general model, ViSQOLAudio [[4]](#references).\n",
    "   - ViSQOL is based on calculating NSIM, basically SSIM applied to neurograms [[6]](#references)\n",
    "   - Model is very similar to ViSQOL (below), but has a VAD for active patch selection and originally used polynomial Q to Q<sub>MOS</sub> mapping [[6], [7]](#references).\n",
    "   - VISQOLAudio is VISQOL modified to evaluate lossy compression. [[4]](#references).\n",
    "   - The introduction of machine learning and output of MOS [[4]](#references) may mean original VISQOL could be better for our use case.\n",
    "   - According to authors of the papers, VISQOLAudio is the first free and open source audio quality metric with accuracy comparable to proprietary metrics used in the industry [[4]](#references).\n",
    "   - The model has four stages: preprocessing, pairing, comparison, and similarity to quality mapping [[4]](#references).\n",
    "      - Preprocessing\n",
    "         - Preprocessing starts with mid channel extraction [[4]](#references). *This step is irrelevant in our case as we intend to only work with mono audio signals.*\n",
    "         - Next step is alignment process, which is meant to remove subframe misalignments caused by encoder padding [[4]](#references).\n",
    "         - Lastly, the \"degraded\" signal is scaled to match reference signal power and Gammatone spectrograms are created [[4]](#references).\n",
    "      - Pairing\n",
    "         - Spectrograms are segmented into patches, ordered set of subsections of the spectrograms in respect to time [[4]](#references).\n",
    "         - Patch pairs with the best similarity obtained using the Neurogram Similarity Index Measure (NSIM) are then found and most similar degraded patch is selected for each reference patch [[4]](#references). The information about which patch pair yield the best result will be used in the last step [[4]](#references) *This step may be problematic when modifying the metric to suit our needs, but as we do not need the similarty measure mapped to MOS, it may be possible to skip it.*\n",
    "      - Comparison\n",
    "         - NSIM is calculated for each patch pair, generating a tensor (NSIM is a vector containing a score for each frequency band) of NSIM scores [[4]](#references).\n",
    "         - Patch pair NSIMs selected in step 2 are averaged to obtain a single value [[4]](#references).\n",
    "      - Similarity to quality mapping\n",
    "         - Similarity score is fed into a support vector regression model to obtain MOS estimate [[4]](#references).\n",
    "\n",
    "- PEAQ [[9]](#references)\n",
    "   - ITU recommendation BS.1387\n",
    "   - Comes in two versions, basic and advanced, where the basic should allow for a cost-efficient real time implementation and the advanced should be more accurate [[9]](#references)\n",
    "   - The algorithm calculates Model Output Variables (MOVs) and estimates MOS based on them [[9]](#references)\n",
    "      - MOVs is a set of extracted features (described later)\n",
    "      - MOS is obtained by feeding MOVs into a neural network\n",
    "   - PEAQ incorporates several other models to obtain the MOVs [[9]](#references)\n",
    "      - DIX\n",
    "      - NMR\n",
    "      - OASE\n",
    "      - PAQM\n",
    "      - PERCEVAL\n",
    "      - POM\n",
    "   - \n",
    "\n",
    "- PAQM\n",
    "\n",
    "- PERCEVAL\n",
    "\n",
    "- POLQA and POLQA Music [[]]()\n",
    "\n",
    "- Comparison of PEAQ x POLQA x PEMO-Q x ViSQOLAudio [[4]](#references)\n",
    "   - From fig. 4 in [[4]](#references) it seems all metrics perform relatively well, but mostly show varying results depending on the used dataset.\n",
    "      - ViSQOLAudio seems to be the most consistent of the tested metrics [[4]](#references).\n",
    "   - PEMO-Q and ViSQOLAudio both seem to generally fit well to the subjective scores [[4]](#references).\n",
    "   - PEAQ consistently gives lower score, especially in AACvOpus15 dataset [[4]](#references).\n",
    "   - POLQA performance depends on the used dataset, struggling on some data and performing well on other [[4]](#references).\n",
    "   - When testing the evaluation metrics without achors:\n",
    "      - ViSQOLAudio \n",
    "      - PEAQ and POLQA have large variation in prediction for high quality samples [[4]](#references).\n",
    "   - Testing suggests PEAQ is sensitive to sample type [[4]](#references).\n",
    "   - POLQA shows poor predictions for high-bitrate samples, which is likely because POLQA being intended for evaluation of speech signals interprets music as noise [[4]](#references).\n",
    "   - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Trained Models\n",
    "\n",
    "- Pros\n",
    "   - perceptual relevance guaranteed by training dataset\n",
    "   - should be easy to use\n",
    "- Cons\n",
    "   - any modification requires re-training the model\n",
    "   - virtually impossible to represent\n",
    "   - no guarantee of performance on data not represented in dataset\n",
    "\n",
    "### Notes from Literature\n",
    "\n",
    "- DPAM and CDPAM [[8]](#references)\n",
    "   - Neural networks trained to perform as perceptual metric [[8]](#references)\n",
    "   - No information about training dataset\n",
    "      - Designed for audio with 22.05 kHz sample rate and compared to speech evaluation metrics only [[8]](#references)\n",
    "      - Likely only suitable for speech signals and thus unsuitable for our needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [1] [A Large-Scale Evaluation of Acoustic and Subjective Music-Similarity Measures](https://www.jstor.org/stable/3681827)\n",
    "- [2] [Embedding a Differentiable MEL-Cepstral Synthesis Filter to a Neural Speech Synthesis System](https://arxiv.org/pdf/2211.11222)\n",
    "- [3] [PEMO-Qâ€”A New Method for Objective Audio Quality Assessment Using a Model of Auditory Perception](https://ieeexplore.ieee.org/document/1709880)\n",
    "- [4] [Objective Assessment of Perceptual Audio Quality Using ViSQOLAudio](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7940042)\n",
    "- [5] [G.107 : The E-model: a computational model for use in transmission planning](https://www.itu.int/rec/T-REC-G.107-201506-I/en)\n",
    "- [6] [ViSQOL: an objective speech quality model](https://asmp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13636-015-0054-9)\n",
    "- [7] [ViSQOL v3: An Open Source Production Ready Objective Speech and Audio Metric](https://arxiv.org/abs/2004.09584)\n",
    "- [8] [A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences](https://arxiv.org/abs/2001.04460)\n",
    "- [9] [BS.1387-2 : Method for objective measurements of perceived audio quality](https://www.itu.int/dms_pubrec/itu-r/rec/bs/R-REC-BS.1387-2-202305-I!!PDF-E.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources to go through\n",
    "\n",
    "- [a] Auditory Feature-based Perceptual Distance\n",
    "  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10925319/\n",
    "  - preprint (not reviewed yet)\n",
    "  - uses a CNN in calculation\n",
    "- [b] Audio retrieval based on perceptual similarity\n",
    "  - https://ieeexplore.ieee.org/document/7014580\n",
    "- [c] Modeling Perceptual Similarity of Audio Signals for Blind Source Separation Evaluation\n",
    "  - https://www.researchgate.net/publication/220848024_Modeling_Perceptual_Similarity_of_Audio_Signals_for_Blind_Source_Separation_Evaluation\n",
    "- [d] A Similarity Measure for Automatic Audio Classification\n",
    "  - likely not too useful\n",
    "  - https://cdn.aaai.org/Symposia/Spring/1997/SS-97-03/SS97-03-001.pdf\n",
    "- [e] An Objective Metric of Human Subjective Audio Quality Optimized for a Wide Range of Audio Fidelities\n",
    "  - https://ieeexplore.ieee.org/abstract/document/4358089\n",
    "- [f] Music Popularity: Metrics, Characteristics, and Audio-Based Prediction\n",
    "  - https://ieeexplore.ieee.org/abstract/document/8327835\n",
    "- [g] POLQA Music\n",
    "  - POLQA meant for evaluation of music signals\n",
    "  - link to be added (can't find more than short reference in one of the ViSQOL papers)\n",
    "- [h] Can we still use PEAQ? A Performance Analysis of the ITU Standard for the Objective Assessment of Perceived Audio Quality\n",
    "  - PEAQ x PEMO-Q x ViSQOLAudio comparison\n",
    "  - https://arxiv.org/pdf/2212.01467\n",
    "- [i] Perceptual Quality Assessment for Digital Audio: PEAQ-The New ITU Standard for Objective Measurement of the Perceived Audio Quality\n",
    "  - https://aes2.org/publications/elibrary-page/?id=8055\n",
    "- Formal listening test reccomendation\n",
    "  - ITU-R BS.1116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help From Others\n",
    "\n",
    "### Metody hodnoceni zvuku od Vaska\n",
    "\n",
    "\n",
    ">Ahoj,\n",
    ">\n",
    ">tady jsou nejake linky na systemy hodnoceni kvality zvuku, ktere by se mohly dat aplikovat na Tvuj problem.\n",
    ">\n",
    ">https://github.com/google/visqol Tohle je system, ktery je podle clanku obstojny a asi jeden z poslednich, o kterych vim. https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7940042\n",
    ">\n",
    ">Tohle je starsi system, ktery ma jednodussi impementaci, takze mozna pro zacatek by byl rozumnejsi https://ieeexplore.ieee.org/document/1709880\n",
    ">\n",
    ">Obecne ty systemy vyuzivaji banky filtru zvanou gammatone filterbank, ktera se da najit treba tady https://amtoolbox.org/models.php Ten zbytek algoritmu by se mel taky dat najit v tom toolboxu a vyhodnoceni uz je pomoci rovnic, co se daji snadno implementovat.\n",
    ">\n",
    ">\n",
    ">Ja sam jsem nikdy prakticky ty systemy nepouzil. Prevzal jsem rozhodovaci cast z PEMO-Q pro jeden konferencni prispevek. Tam jsem nahradil banku filtru necim, co by melo byt vernejsi funkci sluchu, ale zase je to vypocetne nesrovnale narocnejsi. Takze bych sam spise zacal od tech zavedenych signal processingovych postupu v odkazech. Kdyby jsi o funkci sluchu chtel vedet vic, pak mam v doktorske etape predmet.\n",
    ">\n",
    ">Vasek\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
